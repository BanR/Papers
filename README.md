# papers/resources

###Generic:
- https://github.com/aleju/papers/tree/master/neural-nets
- https://github.com/kjw0612/awesome-rnn#image-generation
- https://github.com/jtoy/awesome-tensorflow
- https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap
- https://github.com/rushter/data-science-blogs
- https://github.com/josephmisiti/awesome-machine-learning
- [A guide to convolution arithmetic for deep learning](https://arxiv.org/pdf/1603.07285v1.pdf)
- [AndrewNg cs229](http://cs229.stanford.edu/materials.html)
- tutorial [1](http://ai.stanford.edu/~quocle/tutorial1.pdf) & [2](http://ai.stanford.edu/~quocle/tutorial2.pdf)
- [A Beginner's Guide To Understanding Convolutional Neural Networks](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/)
- [Batch Normalization](https://gab41.lab41.org/batch-normalization-what-the-hey-d480039a9e3b#.mjo2k7r6k)

###Word Embeddings
1. [Deep Learning, NLP, and Representations](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)
2. [Exploiting Similarities among Languages for Machine Translation](https://arxiv.org/pdf/1309.4168.pdf)
3. [Deep Sentence Embedding Using LSTM](https://arxiv.org/pdf/1502.06922v3.pdf)
4. [Distributed Representations of Sentences and Documents](http://cs.stanford.edu/~quocle/paragraph_vector.pdf)
5. [Natural Language Processing (almost) from Scratch](https://arxiv.org/pdf/1103.0398v1.pdf)
6. [word2vec Explained: Deriving Mikolov et al.â€™s Negative-Sampling Word-Embedding Method](https://arxiv.org/pdf/1402.3722v1.pdf)
7. [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)
8. [Neural Word Embedding as Implicit Matrix Factorization](https://levyomer.files.wordpress.com/2014/09/neural-word-embeddings-as-implicit-matrix-factorization.pdf)
9. [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)
10.[Word2Vec Resources](http://mccormickml.com/2016/04/27/word2vec-resources/#efficient-estimation-of-word-representations-in-vector-space)
10. http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/
11. http://blog.datumbox.com/the-dirichlet-process-the-chinese-restaurant-process-and-other-representations/

###Generative Image Models:
1. [Variational Autoencoders](https://arxiv.org/pdf/1312.6114v10.pdf)
  - [Tutorial on Variational Autoencoders](https://arxiv.org/pdf/1606.05908v2.pdf)
  - [Variational Autoencoder in TensorFlow](https://jmetzen.github.io/2015-11-27/vae.html)
2. [Generative Adversarial Networks](https://arxiv.org/pdf/1406.2661v1.pdf)
3. [DCGAN](https://arxiv.org/pdf/1511.06434v2.pdf)
  - http://blog.evjang.com/2016/06/generative-adversarial-nets-in.html
  - Tf: https://github.com/carpedm20/DCGAN-tensorflow
  - Torch : https://github.com/soumith/dcgan.torch  
  - https://github.com/soumith/ganhacks
4. [DRAW](https://arxiv.org/pdf/1502.04623v2.pdf)
  - http://blog.evjang.com/2016/06/understanding-and-implementing.html 
  - https://github.com/ericjang/draw
  - http://kvfrans.com/what-is-draw-deep-recurrent-attentive-writer/
5. [LAPGAN](https://arxiv.org/pdf/1506.05751.pdf)
  - http://www.cs.nyu.edu/~denton/papers/lapgan_supp.pdf
6. [Improved GAN(Open.ai)](https://arxiv.org/pdf/1606.03498v1.pdf)
  - [Code](https://github.com/openai/improved-gan)
7. [Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/pdf/1611.07004v1.pdf)
8. [Image Completion with Deep Learning in TensorFlow](https://bamos.github.io/2016/08/09/deep-completion/)
9. https://openai.com/blog/generative-models/
